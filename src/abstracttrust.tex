\section{Introduction}
  Consider the UC setting, with an environment $\mathcal{E}$, an adversary $\mathcal{A}$ and a set of ITIs that follow a
  given protocol $\Pi$.

  \begin{definition}[Player]
    A player is an ITM that follows $\Pi$. Let $\mathcal{P}$ be the set of all players.
  \end{definition}

  Intuitively, players spontaneously feel different desires of varying intensities and seek to satisfy them, either on their
  own, consuming part of their input tokens in the process, or by delegating the process to other players and paying them for
  their help with part of their input tokens. The choice depends on the perceived difference in price. Each player plays
  rationally, always attempting to maximize her utility.
  
  More precisely, let $\mathcal{D}$ be a (finite) set containing all possible desires. At arbitrary moments during execution,
  $\mathcal{E}$ can provide input to any player $Alice \in \mathcal{P}$ in the form $\left(idx, d, u\right)$, where $idx
  \in \mathbb{N}, d \in \mathcal{D}, u \in \mathbb{R}^{+}$. $idx$ represents an index number that is unique for each input
  $\mathcal{E}$ generates, $d$ represents the desire, whereas $u$ the additional utility $Alice$ will obtain if $d$ is
  satisfied. $d$ is satisfied when $Alice$ learns the string $s\left(idx, Alice, d\right)$, either by directly calculating
  it or by receiving it as subroutine output from another player. Some of the players, given as input the tuple $\left(idx,
  Alice, d\right)$, can calculate $s\left(idx, Alice, d\right)$ more efficiently than $Alice$, which means that they need
  to consume less input tokens than $Alice$ for this calculation. $Alice$ can choose to delegate this calculation to a more
  efficient player $Bob$ and provide the necessary input tokens for his computation with a surplus to compensate $Bob$ for
  his effort. Both players are better off, because $Alice$ spent less tokens than she would if she had calculated
  $s\left(idx, Alice, d\right)$ herself, whilst $Bob$ obtained some tokens which can in turn be used to satisfy some of his
  future desires.

\section{Motivation for our Trust model}
  Nevertheless, one can say that at first sight it is in $Bob$'s best interest to trick $Alice$ into believing that he can
  efficiently calculate $s\left(idx, Alice, d\right)$ and skip the computation entirely after obtaining $Alice$'s input,
  thus keeping all the tokens of the defrauded player. Evidently $Alice$ would avoid further interaction with $Bob$, but
  without any way to signal other players of this unfortunate encounter, $Bob$ can keep defrauding others until the pool of
  players is depleted; if the players are numerous or their number is increasing, $Bob$ may keep this enterprise very
  profitable for an indefinite amount of time. This being a rational strategy, every player would eventually follow it, which
  through a "tragedy of the commons" effect invariably leads to a world where each player must satisfy all her desires by
  herself, entirely missing out on the prospect of division of labor.

  One answer to that undesirable turn of events is a method through which $Alice$, prior to interacting with an aspiring
  helper $Bob$, consults the collective knowledge of her neighborhood of the network regarding him. There are several
  methods to achieve this, such as star-based global ratings. This method however has several drawbacks:

  \begin{itemize}
    \item Very good ratings cost nothing, thus convey little valuable information.
    \item Different players may have different preferences, global ratings fail to capture this.
    \href{https://en.wikipedia.org/wiki/Arrow\%27s_impossibility_theorem}{Arrow's impossibility theorem} is possibly relevant
    here.
    \item Susceptible to Sybil attacks; mitigation techniques are ad-hoc and require (partial) centralization and
    secrecy/obfuscation of methods to succeed, thus undermining the decentralized, transparent nature of the system, a
    property that we actively seek.
  \end{itemize}

\section{Definitions}
  We thus define two kinds of trust: direct and indirect. Direct trust from $Alice$ to $Bob$ is represented by input tokens
  (initially belonging to $Alice$) actively put by her in an account shared with $Bob$. As long as $Bob$ does not take these
  tokens, $Alice$ directly trusts him equally to the amount of tokens in the shared account.
  
  This information can be used by another player $Charlie$ that directly trusts $Alice$ in order to derive information
  regarding $Bob$'s trustworthiness, even if $Charlie$ does not directly trust $Bob$. This is called indirect trust.

    \begin{definition}[Trust]
      
    \end{definition}
    \noindent Let $in = \left(P_1, P_2, GS_1, \{GS'_1, ...,GS'_n\}, t_1, t_2\right) \in \mathcal{P}^2 \times
    \mathcal{S}^{|\mathcal{P}|} \times \mathbb{P}\left(\mathcal{S}^{|\mathcal{P}|}\right) \times \mathbb{N}^2$. Then
    $Tr\left(in\right)$ is interpreted as the level of commitment $P_1$ can provide that the actions of $P_2$ upon a world where
    $GS\left(t_1\right) = GS_1$ will lead to a world where $GS\left(t_2\right) \in \{GS'_1, ...,GS'_n\}$.

    We use the notation $\mathcal{D}_{Tr} = \mathcal{P}^2 \times \mathcal{S}^{|\mathcal{P}|} \times
    \mathbb{P}\left(\mathcal{S}^{|\mathcal{P}|}\right) \times \mathbb{N}^2$.

\subsection{Desired Properties}
  \begin{enumerate}
    \item Let $t \in \mathbb{N}$. Then $\forall \left(P_1, P_2, GS, States, t, t\right) \in \mathcal{D}_{Tr}$ it is
    \begin{equation*}
      Tr\left(P_1, P_2, GS, States, t, t\right) =
	\begin{cases}
	\begin{aligned}
	  \infty, & \mbox{ if } GS \in States \\
        0, & \mbox{ if } GS \notin States
	\end{aligned}
	\end{cases} \enspace.
    \end{equation*}
    In other words, all players trust all other players infinitely with respect to the current state of the world.

    \item Let $t_1, t_2 \in \mathbb{N}: t_1 > t_2$. Then $\forall \left(P_1, P_2, GS, States, t_1, t_2\right) \in
    \mathcal{D}_{Tr}$ it is
    \begin{equation*}
      Tr\left(P_1, P_2, GS, States, t_1, t_2\right) =
	\begin{cases}
	\begin{aligned}
	  \infty, & \mbox{ if } GS\left(t_2\right) \in States \\
        0, & \mbox{ if } GS\left(t_2\right) \notin States
	\end{aligned}
	\end{cases} \enspace.
    \end{equation*}
    This means that the past cannot be modified.

    \item Let $\left(P_1, P_2, GS, States, t_1, t_2\right) \in \mathcal{D}_{Tr}$. If 
    \begin{equation*}
      Tr\left(P_1, P_2, GS, States, t_1, t_2\right) > Tr\left(P_1, P_1, GS, States, t_1, t_2\right)
    \end{equation*}
    and all global states in $States$ are more desirable than $\mathcal{S}^{|\mathcal{P}|} \setminus States$ for $P_1$ at
    the moment $t_2$, then $P_1$ prefers to hand over whatever she controls to $P_2$ at the moment $t_1$ than maintain this
    control for herself.

    \item We can generalize the previous notion as follows: \\ Let $\left(P_1, P_2, GS, States, t_1, t_2\right), \left(P_1,
    P_3, GS, States, t_1, t_2\right) \in \mathcal{D}_{Tr}$. If
    \begin{equation*}
      Tr\left(P_1, P_2, GS, States, t_1, t_2\right) > Tr\left(P_1, P_3, GS, States, t_1, t_2\right)
    \end{equation*}
    and all global states in $States$ are more desirable than $\mathcal{S}^{|\mathcal{P}|} \setminus States$ for $P_1$ at
    the moment $t_2$, then $P_1$ prefers to hand over whatever she controls to $P_2$ at the moment $t_1$ than hand over
    whatever she controls to $P_3$ at the moment $t_1$.
  \end{enumerate}
