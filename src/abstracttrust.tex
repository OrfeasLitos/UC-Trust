\section{Introduction}
  Consider the UC setting, with an environment $\mathcal{E}$, an adversary $\mathcal{A}$ and a set of ITIs that follow a
  given protocol $\Pi$.

  \begin{definition}[Player]
    A player is an ITM that follows $\Pi$. Let $\mathcal{P}$ be the set of all players.
  \end{definition}

  Intuitively, players spontaneously feel different desires of varying intensities and seek to satisfy them, either on their
  own, consuming part of their input tokens in the process, or by delegating the process to other players and paying them for
  their help with part of their input tokens. The choice depends on the perceived difference in price. Each player plays
  rationally, always attempting to maximize her utility.
  
  More precisely, let $\mathcal{D}$ be a (finite) set containing all possible desires. At arbitrary moments during execution,
  $\mathcal{E}$ can provide input to any player $Alice \in \mathcal{P}$ in the form $\left(idx, d, u\right)$, where $idx \in
  \mathbb{N}, d \in \mathcal{D}, u \in \mathbb{R}^{+}$. $idx$ represents an index number that is unique for each input
  $\mathcal{E}$ generates, $d$ represents the desire, whereas $u$ the additional utility $Alice$ will obtain if $d$ is
  satisfied. $d$ is satisfied when $Alice$ learns the string $s\left(idx, d, Alice\right)$, either by directly calculating it
  or by receiving it as subroutine output from another player. Some of the players, given as input the tuple $\left(idx, d,
  Alice\right)$, can calculate $s\left(idx, Alice, d\right)$ more efficiently than $Alice$, which means that they need to
  consume less input tokens than $Alice$ for this calculation. $Alice$ can choose to delegate this calculation to a more
  efficient player $Bob$ and provide the necessary input tokens for his computation with a surplus to compensate $Bob$ for his
  effort. Both players are better off, because $Alice$ spent less tokens than she would if she had calculated $s\left(idx, d,
  Alice\right)$ herself, whilst $Bob$ obtained some tokens which can in turn be used to satisfy some of his future desires.

  \begin{definition}[Cost of desire]
    The cost of $Alice$'s indexed desire, say $\left(idx, d\right) \in \left(\mathbb{N}, \mathcal{D}\right)$, when satisfied by
    $Bob$ is equal to the input tokens that $Alice$ is required by $Bob$ to give to him in order for him to calculate
    $s\left(idx, Alice, d\right)$ and is represented by $c\left(idx, d, Alice, Bob\right)$. The cost of satisfying this desire
    herself is represented by $c\left(idx, d, Alice\right)$ and is equal to the number of computational steps $Alice$ must make
    in order to calculate $s\left(idx, d, Alice\right)$.
  \end{definition}

\section{Motivation for our Trust model}
  Nevertheless, one can say that at first sight it is in $Bob$'s best interest to trick $Alice$ into believing that he can
  efficiently calculate $s\left(idx, Alice, d\right)$ and skip the computation entirely after obtaining $Alice$'s input,
  thus keeping all the tokens of the defrauded player. Evidently $Alice$ would avoid further interaction with $Bob$, but
  without any way to signal other players of this unfortunate encounter, $Bob$ can keep defrauding others until the pool of
  players is depleted; if the players are numerous or their number is increasing, $Bob$ may keep this enterprise very
  profitable for an indefinite amount of time. This being a rational strategy, every player would eventually follow it, which
  through a "tragedy of the commons" effect invariably leads to a world where each player must satisfy all her desires by
  herself, entirely missing out on the prospect of division of labor.

  One answer to that undesirable turn of events is a method through which $Alice$, prior to interacting with an aspiring
  helper $Bob$, consults the collective knowledge of her neighborhood of the network regarding him. There are several
  methods to achieve this, such as star-based global ratings. This method however has several drawbacks:

  \begin{itemize}
    \item Very good ratings cost nothing, thus convey little valuable information.
    \item Different players may have different preferences, global ratings fail to capture this.
    \href{https://en.wikipedia.org/wiki/Arrow\%27s_impossibility_theorem}{Arrow's impossibility theorem} is possibly relevant
    here.
    \item Susceptible to Sybil attacks; mitigation techniques are ad-hoc and require (partial) centralization and
    secrecy/obfuscation of methods to succeed, thus undermining the decentralized, transparent nature of the system, a
    property that we actively seek.
  \end{itemize}

\section{Definitions}
  We thus define two kinds of trust: direct and indirect. Direct trust from $Alice$ to $Bob$ is represented by input tokens
  (initially belonging to $Alice$) actively put by her in an account shared with $Bob$. As long as $Bob$ does not take these
  tokens, $Alice$ directly trusts him equally to the amount of tokens in the shared account.
  
  This information can be used by another player $Charlie$ that directly trusts $Alice$ in order to derive information
  regarding $Bob$'s trustworthiness, even if $Charlie$ does not directly trust $Bob$. This is called indirect trust.

  \begin{definition}[Shared Account]
    An $\left(Alice, Bob\right)$ shared account is an ITI created by $Alice$ that runs the following code:
  \end{definition}
  \Suppressnumber
  \begin{lstlisting}[label=sharedaccount, style=numbers]
((*@$Alice$@*), (*@$Bob$@*)) Shared Account
(*@\Reactivatenumber@*)
Initialization: available-tokens = length(initial input)

Upon receiving input {0,1}(*@$^{\mbox{new-tokens}}$@*) from (*@$Alice$@*)
  available-tokens += new-tokens

Upon receiving message (take, desired-tokens, playerid)
  if (playerid (*@$\in$@*) {(*@$Alice$@*), (*@$Bob$@*)})
    taken-tokens = min(desired-tokens, available-tokens)
    if (taken-tokens > 0)
      available-tokens -= taken-tokens
      input (*@$1^{\mbox{taken-tokens}}$@*) to playerid

Upon receiving message (query, playerid)
  if (playerid (*@$\in$@*) {(*@$Alice$@*), (*@$Bob$@*)})
    send message (available-tokens) to playerid
  \end{lstlisting}
  \begin{definition}[Direct Trust]
    The direct trust from $Alice$ to $Bob$, represented by $DTr_{Alice \rightarrow Bob}$, is zero if the $\left(Alice,
    Bob\right)$ shared account does not exist, otherwise it is equal to the available tokens count sent from this shared
    account an as a response to a message \emph{\texttt{(query, \{}$Alice$\texttt{, }$Bob$\texttt{\})}}.
  \end{definition}

  \begin{definition}[Indirect Trust]
    The indirect trust from $Alice$ to $Bob$, $Tr_{Alice \rightarrow Bob}$, is measured in tokens like direct trust and can be
    calculated deterministically given the existing direct trusts between all players.
  \end{definition}

\section{Desired Properties for Indirect Trust}
  \begin{enumerate}
    \item $Tr_{Alice \rightarrow Bob} \geq DTr_{Alice \rightarrow Bob}$
    \item If universe (1) and (2) are identical except for $DTr_{Alice \rightarrow Bob}$, then
      \begin{equation*}
	Tr^2_{Alice \rightarrow Bob} = Tr^1_{Alice \rightarrow Bob} - DTr^1_{Alice \rightarrow Bob} + DTr^2_{Alice \rightarrow
	Bob} \enspace.
      \end{equation*}
    \item Consider an indexed desire $\left(idx, d\right)$ $Alice$ has. If
    \begin{align*}
      c\left(idx, d, Alice, Bob\right) & < c\left(idx, d, Alice\right) \mbox{ and} \\
      c\left(idx, d, Alice, Bob\right) & \leq Tr_{Alice \rightarrow Bob} \enspace,
    \end{align*}
    then it should be rational for $Alice$ to prefer to delegate the calculation of $s\left(idx, d, Alice\right)$ to $Bob$ than
    to calculate it herself.
  \end{enumerate}

  We would like to provide players with an API where they:
  \begin{enumerate}
    \item Entrust coins to another player
    \item Appropriate coins previously entrusted by another player
    \item Retract coins previously entrusted to another player
    \item Query trust towards another player
  \end{enumerate}
  The following functionality provides such an interface:
  \subimport{./algorithms/}{trustfunc.tex}
